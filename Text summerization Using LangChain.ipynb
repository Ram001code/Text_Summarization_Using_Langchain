{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3089f90a",
   "metadata": {},
   "source": [
    "### Text Summarization Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d80305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai\n",
    "# open_api_key= \"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\"\n",
    "# os.environ[\"OPENAI_API_KEY\"]=open_api_key\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = openai.api_type = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = openai.api_version = \"2022-12-01\" #\"2023-03-15-preview\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = openai.api_base =  \"https://idocopenaigpt.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key = \"95776649ac7a4b048c834003fd315264\"#os.getenv(\"AZUREOPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81447da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Prompt Summarization\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff8f2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech=\"\"\"\n",
    "People across the country, involved in government, political, and social activities, are dedicating their time to make the ‘Viksit Bharat Sankalp Yatra’ (Developed India Resolution Journey) successful. Therefore, as a Member of Parliament, it was my responsibility to also contribute my time to this program. So, today, I have come here just as a Member of Parliament and your ‘sevak’, ready to participate in this program, much like you.\n",
    "\n",
    "In our country, governments have come and gone, numerous schemes have been formulated, discussions have taken place, and big promises have been made. However, my experience and observations led me to believe that the most critical aspect that requires attention is ensuring that the government’s plans reach the intended beneficiaries without any hassles. If there is a ‘Pradhan Mantri Awas Yojana’ (Prime Minister’s housing scheme), then those who are living in jhuggis and slums should get their houses. And he should not need to make rounds of the government offices for this purpose. The government should reach him. Since you have assigned this responsibility to me, about four crore families have got their ‘pucca’ houses. However, I have encountered cases where someone is left out of the government benefits. Therefore, I have decided to tour the country again, to listen to people’s experiences with government schemes, to understand whether they received the intended benefits, and to ensure that the programs are reaching everyone as planned without paying any bribes. We will get the real picture if we visit them again. Therefore, this ‘Viksit Bharat Sankalp Yatra’ is, in a way, my own examination. I want to hear from you and the people across the country whether what I envisioned and the work I have been doing aligns with reality and whether it has reached those for whom it was meant.\n",
    "\n",
    "It is crucial to check whether the work that was supposed to happen has indeed taken place. I recently met some individuals who utilized the Ayushman card to get treatment for serious illnesses. One person met with a severe accident, and after using the card, he could afford the necessary operation, and now he is recovering well. When I asked him, he said: “How could I afford this treatment? Now that there is the Ayushman card, I mustered courage and underwent an operation. Now I am perfectly fine.”  Such stories are blessings to me.\n",
    "\n",
    "The bureaucrats, who prepare good schemes, expedite the paperwork and even allocate funds, also feel satisfied that 50 or 100 people who were supposed to get the funds have got it. The funds meant for a thousand villages have been released. But their job satisfaction peaks when they hear that their work has directly impacted someone’s life positively. When they see the tangible results of their efforts, their enthusiasm multiplies. They feel satisfied. Therefore, ‘Viksit Bharat Sankalp Yatra’ has had a positive impact on government officers. It has made them more enthusiastic about their work, especially when they witness the tangible benefits reaching the people. Officers now feel satisfied with their work, saying, “I made a good plan, I created a file, and the intended beneficiaries received the benefits.” When they find that the money has reached a poor widow under the Jeevan Jyoti scheme and it was a great help to her during her crisis, they realise that they have done a good job. When a government officer listens to such stories, he feels very satisfied.\n",
    "\n",
    "There are very few who understand the power and impact of the ‘Viksit Bharat Sankalp Yatra’. When I hear people connected to bureaucratic circles talking about it, expressing their satisfaction, it resonates with me. I’ve heard stories where someone suddenly received 2 lakh rupees after the death of her husband, and a sister mentioned how the arrival of gas in her home transformed her lives. The most significant aspect is when someone says that the line between rich and poor has vanished. While the slogan ‘Garibi Hatao’ (Remove Poverty) is one thing, but the real change happens when a person says, “As soon as the gas stove came to my house, the distinction between poverty and affluence disappeared.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "803e6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages=[\n",
    "    SystemMessage(content='You are an expert assistant with expertize in summarizing speeches'),\n",
    "    HumanMessage(content=f'Please provide a short and concise summary of the following speech:\\n TEXT: {speech}')\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ebe8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat = AzureChatOpenAI(\n",
    "#     deployment_name= 'chatllm16k',\n",
    "#     temperature=0.7,\n",
    "#     openai_api_version=\"2023-03-15-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f7aac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain\\chat_models\\azure_openai.py:156: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://idocopenaigpt.openai.azure.com/ to https://idocopenaigpt.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain\\chat_models\\azure_openai.py:163: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ramshankar\\anaconda3\\Lib\\site-packages\\langchain\\chat_models\\azure_openai.py:171: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://idocopenaigpt.openai.azure.com/ to https://idocopenaigpt.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm= AzureChatOpenAI(deployment_name= 'chatllm16k',  \n",
    "                     openai_api_version=\"2023-03-15-preview\", \n",
    "                     model_name='gpt-3.5-turbo',\n",
    "                     temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da71c074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Getting total no. of tokens\n",
    "llm.get_num_tokens(speech)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "653e0982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The speech is about the speaker's participation in the 'Viksit Bharat Sankalp Yatra' and the importance of ensuring that government plans reach the intended beneficiaries without any difficulties. The speaker discusses their experience in providing housing to four crore families and their decision to tour the country to understand people's experiences with government schemes. They emphasize the need to check if the work that was supposed to happen has taken place and share stories of individuals benefiting from the Ayushman card. The speaker highlights the impact of the yatra on government officers, as it increases their enthusiasm and satisfaction when they see the tangible benefits reaching people. The speech concludes by mentioning the positive changes experienced by individuals, such as the disappearance of the distinction between poverty and affluence due to the arrival of gas stoves in their homes.\")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(chat_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c6aea",
   "metadata": {},
   "source": [
    "### Prompt Templates Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8252d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9983bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_template='''\n",
    "Write a summary of the following speech:\n",
    "Speech : `{speech}`\n",
    "Translate the precise summary to {language}.\n",
    "\n",
    "'''\n",
    "prompt=PromptTemplate(\n",
    "    input_variables=['speech','language'],\n",
    "    template=generic_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c48f757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWrite a summary of the following speech:\\nSpeech : `\\nPeople across the country, involved in government, political, and social activities, are dedicating their time to make the ‘Viksit Bharat Sankalp Yatra’ (Developed India Resolution Journey) successful. Therefore, as a Member of Parliament, it was my responsibility to also contribute my time to this program. So, today, I have come here just as a Member of Parliament and your ‘sevak’, ready to participate in this program, much like you.\\n\\nIn our country, governments have come and gone, numerous schemes have been formulated, discussions have taken place, and big promises have been made. However, my experience and observations led me to believe that the most critical aspect that requires attention is ensuring that the government’s plans reach the intended beneficiaries without any hassles. If there is a ‘Pradhan Mantri Awas Yojana’ (Prime Minister’s housing scheme), then those who are living in jhuggis and slums should get their houses. And he should not need to make rounds of the government offices for this purpose. The government should reach him. Since you have assigned this responsibility to me, about four crore families have got their ‘pucca’ houses. However, I have encountered cases where someone is left out of the government benefits. Therefore, I have decided to tour the country again, to listen to people’s experiences with government schemes, to understand whether they received the intended benefits, and to ensure that the programs are reaching everyone as planned without paying any bribes. We will get the real picture if we visit them again. Therefore, this ‘Viksit Bharat Sankalp Yatra’ is, in a way, my own examination. I want to hear from you and the people across the country whether what I envisioned and the work I have been doing aligns with reality and whether it has reached those for whom it was meant.\\n\\nIt is crucial to check whether the work that was supposed to happen has indeed taken place. I recently met some individuals who utilized the Ayushman card to get treatment for serious illnesses. One person met with a severe accident, and after using the card, he could afford the necessary operation, and now he is recovering well. When I asked him, he said: “How could I afford this treatment? Now that there is the Ayushman card, I mustered courage and underwent an operation. Now I am perfectly fine.”  Such stories are blessings to me.\\n\\nThe bureaucrats, who prepare good schemes, expedite the paperwork and even allocate funds, also feel satisfied that 50 or 100 people who were supposed to get the funds have got it. The funds meant for a thousand villages have been released. But their job satisfaction peaks when they hear that their work has directly impacted someone’s life positively. When they see the tangible results of their efforts, their enthusiasm multiplies. They feel satisfied. Therefore, ‘Viksit Bharat Sankalp Yatra’ has had a positive impact on government officers. It has made them more enthusiastic about their work, especially when they witness the tangible benefits reaching the people. Officers now feel satisfied with their work, saying, “I made a good plan, I created a file, and the intended beneficiaries received the benefits.” When they find that the money has reached a poor widow under the Jeevan Jyoti scheme and it was a great help to her during her crisis, they realise that they have done a good job. When a government officer listens to such stories, he feels very satisfied.\\n\\nThere are very few who understand the power and impact of the ‘Viksit Bharat Sankalp Yatra’. When I hear people connected to bureaucratic circles talking about it, expressing their satisfaction, it resonates with me. I’ve heard stories where someone suddenly received 2 lakh rupees after the death of her husband, and a sister mentioned how the arrival of gas in her home transformed her lives. The most significant aspect is when someone says that the line between rich and poor has vanished. While the slogan ‘Garibi Hatao’ (Remove Poverty) is one thing, but the real change happens when a person says, “As soon as the gas stove came to my house, the distinction between poverty and affluence disappeared.\\n`\\nTranslate the precise summary to Hindi.\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(speech=speech,language='Hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a23e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning varible for complete prompt\n",
    "\n",
    "complete_prompt=prompt.format(speech=speech,language='Hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8898421b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CHecking the numbers of tokens \n",
    "\n",
    "llm.get_num_tokens(complete_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "358ab977",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "summary = llm_chain.run({'speech':speech,'language':'hindi'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2eecc27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'भाषणः\\nदेशभर में सरकार, राजनीतिक और सामाजिक गतिविधियों में जुटे लोग विकसित भारत संकल्प यात्रा को सफल बनाने के लिए अपना समय समर्पित कर रहे हैं। इसलिए, सांसद के रूप में मेरी जिम्मेदारी भी इस कार्यक्रम में समय देने की थी। तो आज मैं यहां सिर्फ एक सांसद और आपका सेवक के रूप में आया हूं, तैयार हूं इस कार्यक्रम में भाग लेने के लिए, आपकी तरह।\\n\\nहमारे देश में सरकारें आई हैं और चली गई हैं, अनेक योजनाएं तैयार की गईं हैं, चर्चाएं हुई हैं और भविष्यवाणियों की गई हैं। हालांकि, मेरे अनुभव और निरीक्षण ने मुझे यह मानने के लिए प्रेरित किया है कि सबसे महत्वपूर्ण पहलू यह है कि सरकार की योजनाएं इच्छित लाभार्थियों तक बिना किसी परेशानी के पहुंचे। अगर \\'प्रधानमंत्री आवास योजना\\' है, तो जिन लोगों की झुग्गियों और स्लम में रहने की स्थिति है, उन्हें अपने घर मिलने चाहिए। और उसे उसके लिए सरकारी कार्यालयों के चक्कर नहीं काटने की जरूरत होनी चाहिए। सरकार को उसके पास पहुंचना चाहिए। जबसे आपने मुझे इस जिम्मेदारी को सौंपा है, तबसे लगभग चार करोड़ परिवारों को उनके पक्के घर मिल चुके हैं। हालांकि, मैंने ऐसे मामलों का सामना किया है जहां किसी को सरकारी लाभ से वंचित छोड़ दिया गया है। इसलिए, मैंने फिर से देश की यात्रा करने का फैसला किया है, लोगों के सरकारी योजनाओं के अनुभव सुनने के लिए, समझने के लिए कि क्या उन्हें इच्छित लाभ प्राप्त हुआ है और कार्यक्रम सभी को योजना के अनुसार पहुंच रहा है बिना किसी रिश्वत के। हमें अगर फिर से उनसे मिलने जाएं तो हमें वास्तविक तस्वीर मिलेगी। इसलिए, यह \\'विकसित भारत संकल्प यात्रा\\' अपनी तरह मेरी खुद की परीक्षा है। मैं आपसे और देशभर के लोगों से जानना चाहता हूं कि क्या मैने जो सोचा था और जो काम किया है, वह वास्तविकता के साथ मेल खाता है और क्या उन लोगों तक पहुंचा है जिनके लिए यह योजना थी।\\n\\nयह महत्वपूर्ण है कि काम जो होना चाहिए, क्या वास्तव में हुआ है, यह जांचना। हाल ही में मैंने कुछ लोगों से मिले जिन्होंने गंभीर बीमारियों के इलाज के लिए आयुष्मान कार्ड का उपयोग किया। एक व्यक्ति को गंभीर हादसा हुआ था और कार्ड का उपयोग करने के बाद, उसे आवश्यक ऑपरेशन करने की सामर्थ्य मिली, और अब वह ठीक हो रहा है। जब मैंने उससे पूछा, तो उसने कहा: \"मैं इस इलाज को कैसे कर सकता था? अब जब आयुष्मान कार्ड है, तो मैंने साहस जुटाया और एक ऑपरेशन करवाया। अब मैं पूरी तरह सही हूं।\" ऐसे कहानियाँ मेरे लिए आशीर्वाद हैं।\\n\\nब्यूरोक्रेट, जो अच्छी योजनाएं तैयार करते हैं, कागजात को तेजी से प्रस्तुत करते हैं और धन भी आवंटित करते हैं, उन्हें भी संतुष्टि महसूस होती है कि 50 या 100 लोग, जिन्हें धन प्राप्त होना चाहिए था, उसे प्राप्त कर चुके हैं। हजारों गांवों के लिए धन जारी कर दिया गया है। लेकिन उनकी नौकरी की संतुष्टि जब वे सुनते हैं कि उनका काम किसी की जीवन में सीधा प्रभाव डाल रहा है, तब वे ऊर्जावान हो जाते हैं। जब वे अपने प्रयासों के वास्तविक परिणाम देखते हैं, तो उनकी उत्साहिता बढ़ जाती है। वे संतुष्ट महसूस करते हैं। इसलिए, \\'विकसित भारत संकल्प यात्रा\\' ने सरकारी अधिकारियों पर सकारात्मक प्रभाव डाला है। यह उन्हें अपने काम के प्रति अधिक उत्साही बना दिया है, खासकर जब उन्हें देखते हैं कि वास्तविक लाभ लोगों तक पहुंच रहा है। अधिकारी अब अपने काम से संतुष्ट हो रहे हैं, कहते हुए, \"मैंने एक अच्छी योजना बनाई, मैंने एक फ़ाइल बनाई, और इच्छित लाभ प्राप्त करने वाले लोगों को लाभ मिला।\" जब वह देखते हैं कि पूजा ज्योति योजना के तहत एक गरीब विधवा को पैसे पहुंचे हैं और यह उसकी संकट के समय बहुत मददगार रहा, तो उन्हें लगता है कि उन्होंने एक अच्छा काम किया है। जब एक सरकारी अधिकारी ऐसी कहानियाँ सुनता है, तो उसे बहुत संतुष्टि महसूस होती है।\\n\\n\\'विकसित भारत संकल्प यात्रा\\' की शक्ति और प्रभाव को समझने वाले बहुत कम हैं। जब मैं अधिकारीजनों के साथ बातचीत करते सुनता हूं, उनकी संतुष्टि का व्यक्त करते हुए, तो वह मुझसे मिलता है। मैंने कहानियाँ सुनी हैं जहां किसी को अचानक उसके पति की मृत्यु के बाद 2 लाख रुपए मिले, और एक बहन ने बताया कि उनके घर में गैस के आगमन ने उनके जीवन को परिवर्तित कर दिया। सबसे महत्वपूर्ण हिस्सा यह है जब कोई कहता है कि अमीर और गरीब के बीच की रेखा गायब हो गई है। \\'गरीबी हटाओ\\' का नारा तो एक बात है, लेकिन वास्तविक परिवर्तन तब होता है जब कोई व्यक्ति कहता है, \"मेरे घर गैस का चूल्हा आते ही, गरीबी और सम्पन्नता के बीच का अंत गायब हो गया है।\"'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a034c5",
   "metadata": {},
   "source": [
    "### StuffDocumentChain Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "957c8a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "     ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "     ------ ------------------------------ 41.0/232.6 kB 653.6 kB/s eta 0:00:01\n",
      "     -------------------------- ----------- 163.8/232.6 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 232.6/232.6 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c3b7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "174c02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the path of  pdf file/files.\n",
    "pdfreader = PdfReader('cosine-similarity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "baebfa46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PyPDF2._page._VirtualList at 0x23e3f098150>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfreader.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "193bb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "\n",
    "text= ''\n",
    "\n",
    "for i, info in enumerate(pdfreader.pages):\n",
    "    cont= info.extract_text()\n",
    "    if cont:\n",
    "        text +=cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51aad5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/327248753\\nCosine Similarity T utorial\\nArticle  · April 2015\\nCITATIONS\\n48READS\\n2,081\\n1 author:\\nEdel Gar cia\\nMiner azzi. com\\n15 PUBLICA TIONS \\xa0\\xa0\\xa069 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Edel Gar cia on 27 A ugust 2018.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.Cosine Similarity  T utorial  \\n \\nAbstract – This is a tutorial on the cosine similarity measure . Its meaning in the context of uncorrelated and orthogonal \\nvariables is examined.  \\n \\nKeywords: cosine similarity, tutorial, dot products, vectors , orthogonal, uncorrelated  \\nPublished : 04-10-2015 ; Update d: 08-26-2018  \\n© Edel  Garcia , PhD ; admin@minerazzi.com   \\n \\nIntroduction  \\nIn previous tutorials we discussed  the difference between distance and similarit y measures  and the \\nrisks of arbitrarily transforming or averaging these ( Garcia, 2015a;  2015b;  2015c;  2015d ).  \\nWe mentioned that  a Pearson’ s Correlation Coefficient  (r) computed from mean -centered \\nvariables , or from z-scores , is a cosine similarity . In this case , r can be computed from the \\nregression curve  slope .  \\nWe also  mentioned that l ike correlations , slopes, and cosines,  cosine similarity measures are \\nnot additive  so we cannot compute  arithmetic average s from any of these measures . The same can \\nbe said about standard deviations, rates, and dissimilar ratios. It should be noted that a mean value \\nis not an estimate of central tendency when a distribu tion is either skewed or Cauchy . Furthermore,  \\nthe Law of Large Numbers  does not apply  to a Cauchy Distribution . \\n \\nConventions used in this tutorial  \\nThis tutorial was written as  a companion for our Cosine Similarity Calculator (Garcia, 2015a).  \\nIt may serve as a basic tutorial for students and those  starting in data mining  and information \\nretrieval. F or the sake of clarity , we adopt the following conventions:  \\n \\n\\uf0b7 a and b are th e    and     vectors.  \\n\\uf0b7 dpab is the         dot product between a and b. \\n\\uf0b7 dpaa is the dot product of a with itself.  \\n\\uf0b7 dpbb is the dot product of b with itself . \\n\\uf0b7 la and lb are the      and       vector lengths .  \\n\\uf0b7    and    are unit vectors; i.e.,  la = lb = 1. 2 \\n \\nInstead of just saying that the  cosine similarity between two vectors is given by the expression  \\n \\n             \\n                              (1) \\n \\nwe want to explain  what is actually scored  with (1).  \\nSimilarity is an interesting measure as there are many ways of computing it. Indeed, we built a \\ntool that computes over 70 different similarity measures  (Garcia , 2016). Since there are so many  \\nways of expressing similarity , what kind of resemblance a cosine similarity  actually scores ? This is \\nthe question that this tutorial pretends to address .  \\nGenerally speaking, similarity is a measure of resemblance; i.e., how similar or alike things \\nbeing compared are . One way  of computing similarity is through the use of vectors.   \\n \\nRepresenting data sets as vectors  \\nA matrix  is just a table filled with values . Suppose that said table consists of r number of rows and \\nc number of columns . We may refer to these as  vectors . Thus for a square matrix, one with same \\nnumber of rows and columns, row vectors  are data sets of size n = c  and column vectors  are data \\nsets of size n = r .  \\nA vector is a quantity or phenomenon  with two independent properties: direction and \\nmagnitude . For n = 2 and  n = 3, we can visualize  a vector in its n-dimensional space as a line \\nsegment ending in an arrow. The orientation of the  line segment is its direction , and the length is its \\nmagnitude.  For n >  3 we cannot visualize vectors, but we can still compute them.  \\n \\nComputing Dot Products  \\nWe may multiply a vector by itself or a nother  vector and compute a quantity called the dot p roduct  \\n(dp). This is done by multiplying vector elements  and taking summations.  To illus trate, s uppose \\nthat a and b are vectors  such that  \\n \\n\\uf0b7 a = [1, 2, 3]  \\n\\uf0b7 b = [4, -5, 6] \\n 3 \\n \\nIn this case, the dot product of  \\n \\n\\uf0b7 a with b is dpab = 1*4 + 2* -5 + 3*6 = 12  \\n\\uf0b7 a with itself is dpaa = 1*1 + 2*2 + 3*3 = 14  \\n\\uf0b7 b with itself is dpbb = 4*4 + -5*-5 + 6*6 = 77  \\n \\nWe may  now ask : What kind of information is stored in dpab, dpaa, and dpbb? \\n \\nWhat information is stored in dpab?  \\nGood question: dpab holds information about the direction  of the vectors . To be precise, if  \\n \\n\\uf0b7 dpab > 0, a and b form an angle less than 90o. \\n\\uf0b7 dpab = 0, a and b form an angle that is exactly  90o. \\n\\uf0b7 dpab < 0, a and b form an angle greater than 90o. \\n \\nTable 1 shows different types of angles. \\n \\nTable 1. Possible types of angles.  \\nType  acute  right obtuse  straight  reflex  perigon  \\nAngle , \\uf071 \\uf071\\uf020\\uf020\\uf03c\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf03d\\uf020\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf031\\uf038\\uf030\\uf020\\uf03e\\uf020\\uf071\\uf020\\uf03e\\uf020\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf020\\uf03d\\uf020\\uf031\\uf038\\uf030\\uf06f\\uf020\\uf033\\uf036\\uf030\\uf06f\\uf020\\uf03e\\uf020\\uf071\\uf020\\uf03e\\uf020\\uf020\\uf031\\uf038\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf03d\\uf020\\uf033\\uf036\\uf030\\uf06f\\uf020\\n \\nAn angle of 0o means that cos \\uf071 = 1 so the vectors point to  identical directions. A n angle of 90o \\nmeans that cos \\uf071 = 0 so t he vectors point to  perpendicular directions or are orthogonal .  \\n \\nWhat information is stored in dpaa and dpbb? \\nTaking square roots, it is clear that  dpaa and dpbb hold length informatio n. So \\n \\n\\uf0b7 la = (dpaa)½ = (14)½ = 3.74     ; i.e., the length of a. \\n\\uf0b7 lb = (dpbb)½ = (77)½ = 8.77    ; i.e., the length of  b. \\n\\uf0b7 la*lb = (dpaa)½ * (dpbb)½ = 32.83   ; i.e., the length product (lpab) of a and b. 4 \\n \\nIn other words,  (1) is a dot product/length product ratio  \\n \\n370=843212= = ..=θcosdp dpdp\\nlpdp\\nbb aaab\\nabab\\n              (2) \\n \\nTherefore, when we compute a cosine similarity  we are measuring the direction -lengt h  \\nresemblance between data sets  represented as vectors .  \\n \\nComparing V ectors of Different Lengths  \\nTo compare vectors  of different lengths , these can be recomputed  as unit vectors . A unit vector  is \\ncomputed  by dividing its elements by its  length. In other words, we are re  the previous vectors  as \\n \\n    = [1/3.74, 2/3.74, 3/3.74] \\n    = [4/8.77, -5/8.77, 6/8.77 ]  \\n \\nwhere the  hat (^) denotes a unit vector . Since the new lengths are equal to 1, the cosine similarity \\nbetween     and     is their dot product ; hence   \\n \\n  \\n370= = . θcos dpbˆaˆ                     (3) \\n \\nExpressions (1), (2), and (3) return  the same result, confirming that a cosine  is a judgment of  \\nthe orient ation of the vectors , independent of their lengths  (Wikipedia, 2015 a). \\n \\nWhat does cos \\uf071 = 0 mean ? \\nAs mentioned before, an angle of 90o means that cos \\uf071 = 0 so the vectors are perpendicular or \\northogonal. This does not necessarily mean that the variables are uncorrelated .  \\nAccording to Rodgers, Nicewander, & Toothaker (1984)  when referring to variables,  \\northogonal denotes  that raw variables are perpe ndicular while  uncorrelated  means  that centered  \\nvariables are perpendicular. Centered variables are those  with their mean removed, so  they have \\nzero mean.  5 \\n \\nIt turns out that un like vector length normalization, subtracting the mean from raw variables  \\ncan change the angle between the  vectors . The following can happen to the raw variables :  \\n \\n\\uf0b7 If they are perpendicular , can become not perpendicular so they are orthogonal, but not \\nuncorrelated.  \\n\\uf0b7 If they are not perpendicular , can become perpendicular so they are uncorrelated, but not \\northogonal.  \\n\\uf0b7 If they are perpendicular  and remain perpendicular , they are orthogonal and uncorrelated.  \\n\\uf0b7 If they are not perpendicular and remain not perpendicular , they are neither orthogonal nor \\nuncorrelated —although their angle can change . \\n \\nThe following figure , adapted from Rodgers et. al  (1984 ), illustrates these relationships .  \\nNotice that not all uncorrelated variables are orthogonal and vice versa  (Wikipedia, 2015b). The \\nfigure also shows that while all uncorrelated variables are independent, the reverse is not true.  \\nTherefore, if a textbook states that orthogo nal variables are uncorrelated, it probab ly refers to \\npaired random variables with  zero mean; i.e., to the overlapping region shown in the figure.   \\n \\n \\n \\n \\nFigure 1. Venn Diagram for  linearly  independent , uncorrelated, and orthogonal variables . \\n \\n \\n6 \\n \\nThese results are not surprising. It can be shown for centered variables that  \\n \\nr = cos \\uf071                       (4) \\n \\nBecause  r is the covariance  of paired variables n ormalize d by their standard  deviation,  \\n \\n \\nsscov\\nbaabr=                       (5) \\n \\nit must follow for centered  variables that  if cos \\uf071 = 0, then r = 0 and cov ab = 0.  \\nThe relationship r = cos \\uf071 is also true for standardized variables , also known as z-scores . These \\nare centered variables normali zed by their standard deviation. Transforming variables into z-score s \\nis, though not always, useful and recommended.   \\nOnce in a z-score format, we can run other types of tests on the variables like a quantile - \\nquantile analysis (Garcia, 2015e), or even compute r, and therefore cos \\uf071, from the slope of the \\nregression curve of  z-scores . For those inter ested , we have developed  another  \\ntool called the Standardizer (Garcia, 2015f). This tool was initially thought of as an x-to-z-score \\nstandardizer, hence its name. It is now a versatile statistical tool for univariate analysis.  \\n \\nConclusion  \\nUnlike other similarity measures , a cosine similarity is a measure of the direction -length  \\nresemblance between vectors.   \\nAn angle of 0o means that cos \\uf071 = 1 and that the vectors are oriented in identical directions; i.e.,  \\nthat the corresponding data sets  are completely similar to one another.  An angle of 90o means that \\ncos \\uf071 = 0 and that the corresponding variables are perpendicular , but not necessarily that are \\nuncorrelated .  \\nComputing cos \\uf071 from raw and center variables are two different things. T his fact  \\ncan be used to examine relationships between paired variables . Regardless of the method used to \\ncompute cosine similar ities, and for the sake of transparency , it is always a good idea to state \\nwhether raw or centered variables were used and why .  7 \\n \\nReferences  \\nGarcia, E. (2016 ). Binary Similarity Calculator. Retrieved from  \\nhttp://www.minerazzi.com/tools/s imilarity/binary -similarity -calculator.php   \\n \\nGarcia, E. (2015a) . Cosine Similarity Cal culator. Retrieved from  \\nhttp://www.minerazzi.com/tools/cosine -similarity/cosine -similarity -calculator.php   \\n \\nGarcia, E. (2015b). A Tutorial on Distance and Similarity . Retrieved from  \\nhttp://www.minerazzi.com/tutorials/distance -similarity -tutorial.pdf  \\n \\nGarcia, E. (2015 c). The Self -Weighting Model Tutorial: Part 1 Retrieved from  \\nhttp://www.minerazzi.com/tutorials/self -weighting -model -tutorial -part-1.pdf  \\n \\nGarcia, E. (2015d ). The Self -Weighting Model Tutorial: Part 2 Retrieved from  \\nhttp://www.m inerazzi.com/tutorials/self -weighting -model -tutorial -part-2.pdf  \\n  \\nGarcia, E. (2015e). A Quantile -Quantile Plot tutorial. Retrieved from  \\nhttp://www.minerazzi.com/tutorials/quant ile-quantile -tutorial.pdf  \\n \\nGarcia, E. (2015f). The Standardizer. Retrieved from  \\nhttp://www.minerazzi.com/tools/standardizer/z -scores.php  \\n \\nRodgers, J. L., Nicewander, W. A., Toothaker, L. (1984). Linearly Independent, Orthogonal, and  \\nUncorrelated V ariables. The American Statistician , V ol. 38, No. 2. Pp 133 -134. Retrieved from  \\nhttps://pdfs.semanticscholar.org/177d/c3b719fdb7b2c01d6c341e0bd58a09a47a83.pdf   \\n \\nWikipedia (2015 a). Cosine Similarity . Retrieved from  \\nhttp://en.wikipedia.org/wiki/Cosi ne_similarity  \\n \\nWikipedia (2015b). Uncorrelated. Retrieved from  \\nhttp://en.wikipedia.org/wiki/Uncorrelated  \\nView publication stats'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4502283b",
   "metadata": {},
   "source": [
    "## another way to read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "683a7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d159d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PyPDFLoader('cosine-similarity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c823c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.document_loaders.pdf.PyPDFLoader at 0x23e3f006310>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a55eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pdf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f5a8d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/327248753\\nCosine Similarity T utorial\\nArticle  · April 2015\\nCITATIONS\\n48READS\\n2,081\\n1 author:\\nEdel Gar cia\\nMiner azzi. com\\n15 PUBLICA TIONS \\xa0\\xa0\\xa069 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Edel Gar cia on 27 A ugust 2018.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', metadata={'source': 'cosine-similarity.pdf', 'page': 0}),\n",
       " Document(page_content='Cosine Similarity  T utorial  \\n \\nAbstract – This is a tutorial on the cosine similarity measure . Its meaning in the context of uncorrelated and orthogonal \\nvariables is examined.  \\n \\nKeywords: cosine similarity, tutorial, dot products, vectors , orthogonal, uncorrelated  \\nPublished : 04-10-2015 ; Update d: 08-26-2018  \\n© Edel  Garcia , PhD ; admin@minerazzi.com   \\n \\nIntroduction  \\nIn previous tutorials we discussed  the difference between distance and similarit y measures  and the \\nrisks of arbitrarily transforming or averaging these ( Garcia, 2015a;  2015b;  2015c;  2015d ).  \\nWe mentioned that  a Pearson’ s Correlation Coefficient  (r) computed from mean -centered \\nvariables , or from z-scores , is a cosine similarity . In this case , r can be computed from the \\nregression curve  slope .  \\nWe also  mentioned that l ike correlations , slopes, and cosines,  cosine similarity measures are \\nnot additive  so we cannot compute  arithmetic average s from any of these measures . The same can \\nbe said about standard deviations, rates, and dissimilar ratios. It should be noted that a mean value \\nis not an estimate of central tendency when a distribu tion is either skewed or Cauchy . Furthermore,  \\nthe Law of Large Numbers  does not apply  to a Cauchy Distribution . \\n \\nConventions used in this tutorial  \\nThis tutorial was written as  a companion for our Cosine Similarity Calculator (Garcia, 2015a).  \\nIt may serve as a basic tutorial for students and those  starting in data mining  and information \\nretrieval. F or the sake of clarity , we adopt the following conventions:  \\n \\n\\uf0b7 a and b are th e    and     vectors.  \\n\\uf0b7 dpab is the         dot product between a and b. \\n\\uf0b7 dpaa is the dot product of a with itself.  \\n\\uf0b7 dpbb is the dot product of b with itself . \\n\\uf0b7 la and lb are the      and       vector lengths .  \\n\\uf0b7    and    are unit vectors; i.e.,  la = lb = 1. ', metadata={'source': 'cosine-similarity.pdf', 'page': 1}),\n",
       " Document(page_content='2 \\n \\nInstead of just saying that the  cosine similarity between two vectors is given by the expression  \\n \\n             \\n                              (1) \\n \\nwe want to explain  what is actually scored  with (1).  \\nSimilarity is an interesting measure as there are many ways of computing it. Indeed, we built a \\ntool that computes over 70 different similarity measures  (Garcia , 2016). Since there are so many  \\nways of expressing similarity , what kind of resemblance a cosine similarity  actually scores ? This is \\nthe question that this tutorial pretends to address .  \\nGenerally speaking, similarity is a measure of resemblance; i.e., how similar or alike things \\nbeing compared are . One way  of computing similarity is through the use of vectors.   \\n \\nRepresenting data sets as vectors  \\nA matrix  is just a table filled with values . Suppose that said table consists of r number of rows and \\nc number of columns . We may refer to these as  vectors . Thus for a square matrix, one with same \\nnumber of rows and columns, row vectors  are data sets of size n = c  and column vectors  are data \\nsets of size n = r .  \\nA vector is a quantity or phenomenon  with two independent properties: direction and \\nmagnitude . For n = 2 and  n = 3, we can visualize  a vector in its n-dimensional space as a line \\nsegment ending in an arrow. The orientation of the  line segment is its direction , and the length is its \\nmagnitude.  For n >  3 we cannot visualize vectors, but we can still compute them.  \\n \\nComputing Dot Products  \\nWe may multiply a vector by itself or a nother  vector and compute a quantity called the dot p roduct  \\n(dp). This is done by multiplying vector elements  and taking summations.  To illus trate, s uppose \\nthat a and b are vectors  such that  \\n \\n\\uf0b7 a = [1, 2, 3]  \\n\\uf0b7 b = [4, -5, 6] \\n ', metadata={'source': 'cosine-similarity.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\n \\nIn this case, the dot product of  \\n \\n\\uf0b7 a with b is dpab = 1*4 + 2* -5 + 3*6 = 12  \\n\\uf0b7 a with itself is dpaa = 1*1 + 2*2 + 3*3 = 14  \\n\\uf0b7 b with itself is dpbb = 4*4 + -5*-5 + 6*6 = 77  \\n \\nWe may  now ask : What kind of information is stored in dpab, dpaa, and dpbb? \\n \\nWhat information is stored in dpab?  \\nGood question: dpab holds information about the direction  of the vectors . To be precise, if  \\n \\n\\uf0b7 dpab > 0, a and b form an angle less than 90o. \\n\\uf0b7 dpab = 0, a and b form an angle that is exactly  90o. \\n\\uf0b7 dpab < 0, a and b form an angle greater than 90o. \\n \\nTable 1 shows different types of angles. \\n \\nTable 1. Possible types of angles.  \\nType  acute  right obtuse  straight  reflex  perigon  \\nAngle , \\uf071 \\uf071\\uf020\\uf020\\uf03c\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf03d\\uf020\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf031\\uf038\\uf030\\uf020\\uf03e\\uf020\\uf071\\uf020\\uf03e\\uf020\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf020\\uf03d\\uf020\\uf031\\uf038\\uf030\\uf06f\\uf020\\uf033\\uf036\\uf030\\uf06f\\uf020\\uf03e\\uf020\\uf071\\uf020\\uf03e\\uf020\\uf020\\uf031\\uf038\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf03d\\uf020\\uf033\\uf036\\uf030\\uf06f\\uf020\\n \\nAn angle of 0o means that cos \\uf071 = 1 so the vectors point to  identical directions. A n angle of 90o \\nmeans that cos \\uf071 = 0 so t he vectors point to  perpendicular directions or are orthogonal .  \\n \\nWhat information is stored in dpaa and dpbb? \\nTaking square roots, it is clear that  dpaa and dpbb hold length informatio n. So \\n \\n\\uf0b7 la = (dpaa)½ = (14)½ = 3.74     ; i.e., the length of a. \\n\\uf0b7 lb = (dpbb)½ = (77)½ = 8.77    ; i.e., the length of  b. \\n\\uf0b7 la*lb = (dpaa)½ * (dpbb)½ = 32.83   ; i.e., the length product (lpab) of a and b. ', metadata={'source': 'cosine-similarity.pdf', 'page': 3}),\n",
       " Document(page_content='4 \\n \\nIn other words,  (1) is a dot product/length product ratio  \\n \\n370=843212= = ..=θcosdp dpdp\\nlpdp\\nbb aaab\\nabab\\n              (2) \\n \\nTherefore, when we compute a cosine similarity  we are measuring the direction -lengt h  \\nresemblance between data sets  represented as vectors .  \\n \\nComparing V ectors of Different Lengths  \\nTo compare vectors  of different lengths , these can be recomputed  as unit vectors . A unit vector  is \\ncomputed  by dividing its elements by its  length. In other words, we are re  the previous vectors  as \\n \\n    = [1/3.74, 2/3.74, 3/3.74] \\n    = [4/8.77, -5/8.77, 6/8.77 ]  \\n \\nwhere the  hat (^) denotes a unit vector . Since the new lengths are equal to 1, the cosine similarity \\nbetween     and     is their dot product ; hence   \\n \\n  \\n370= = . θcos dpbˆaˆ                     (3) \\n \\nExpressions (1), (2), and (3) return  the same result, confirming that a cosine  is a judgment of  \\nthe orient ation of the vectors , independent of their lengths  (Wikipedia, 2015 a). \\n \\nWhat does cos \\uf071 = 0 mean ? \\nAs mentioned before, an angle of 90o means that cos \\uf071 = 0 so the vectors are perpendicular or \\northogonal. This does not necessarily mean that the variables are uncorrelated .  \\nAccording to Rodgers, Nicewander, & Toothaker (1984)  when referring to variables,  \\northogonal denotes  that raw variables are perpe ndicular while  uncorrelated  means  that centered  \\nvariables are perpendicular. Centered variables are those  with their mean removed, so  they have \\nzero mean.  ', metadata={'source': 'cosine-similarity.pdf', 'page': 4}),\n",
       " Document(page_content='5 \\n \\nIt turns out that un like vector length normalization, subtracting the mean from raw variables  \\ncan change the angle between the  vectors . The following can happen to the raw variables :  \\n \\n\\uf0b7 If they are perpendicular , can become not perpendicular so they are orthogonal, but not \\nuncorrelated.  \\n\\uf0b7 If they are not perpendicular , can become perpendicular so they are uncorrelated, but not \\northogonal.  \\n\\uf0b7 If they are perpendicular  and remain perpendicular , they are orthogonal and uncorrelated.  \\n\\uf0b7 If they are not perpendicular and remain not perpendicular , they are neither orthogonal nor \\nuncorrelated —although their angle can change . \\n \\nThe following figure , adapted from Rodgers et. al  (1984 ), illustrates these relationships .  \\nNotice that not all uncorrelated variables are orthogonal and vice versa  (Wikipedia, 2015b). The \\nfigure also shows that while all uncorrelated variables are independent, the reverse is not true.  \\nTherefore, if a textbook states that orthogo nal variables are uncorrelated, it probab ly refers to \\npaired random variables with  zero mean; i.e., to the overlapping region shown in the figure.   \\n \\n \\n \\n \\nFigure 1. Venn Diagram for  linearly  independent , uncorrelated, and orthogonal variables . \\n \\n \\n', metadata={'source': 'cosine-similarity.pdf', 'page': 5}),\n",
       " Document(page_content='6 \\n \\nThese results are not surprising. It can be shown for centered variables that  \\n \\nr = cos \\uf071                       (4) \\n \\nBecause  r is the covariance  of paired variables n ormalize d by their standard  deviation,  \\n \\n \\nsscov\\nbaabr=                       (5) \\n \\nit must follow for centered  variables that  if cos \\uf071 = 0, then r = 0 and cov ab = 0.  \\nThe relationship r = cos \\uf071 is also true for standardized variables , also known as z-scores . These \\nare centered variables normali zed by their standard deviation. Transforming variables into z-score s \\nis, though not always, useful and recommended.   \\nOnce in a z-score format, we can run other types of tests on the variables like a quantile - \\nquantile analysis (Garcia, 2015e), or even compute r, and therefore cos \\uf071, from the slope of the \\nregression curve of  z-scores . For those inter ested , we have developed  another  \\ntool called the Standardizer (Garcia, 2015f). This tool was initially thought of as an x-to-z-score \\nstandardizer, hence its name. It is now a versatile statistical tool for univariate analysis.  \\n \\nConclusion  \\nUnlike other similarity measures , a cosine similarity is a measure of the direction -length  \\nresemblance between vectors.   \\nAn angle of 0o means that cos \\uf071 = 1 and that the vectors are oriented in identical directions; i.e.,  \\nthat the corresponding data sets  are completely similar to one another.  An angle of 90o means that \\ncos \\uf071 = 0 and that the corresponding variables are perpendicular , but not necessarily that are \\nuncorrelated .  \\nComputing cos \\uf071 from raw and center variables are two different things. T his fact  \\ncan be used to examine relationships between paired variables . Regardless of the method used to \\ncompute cosine similar ities, and for the sake of transparency , it is always a good idea to state \\nwhether raw or centered variables were used and why .  ', metadata={'source': 'cosine-similarity.pdf', 'page': 6}),\n",
       " Document(page_content='7 \\n \\nReferences  \\nGarcia, E. (2016 ). Binary Similarity Calculator. Retrieved from  \\nhttp://www.minerazzi.com/tools/s imilarity/binary -similarity -calculator.php   \\n \\nGarcia, E. (2015a) . Cosine Similarity Cal culator. Retrieved from  \\nhttp://www.minerazzi.com/tools/cosine -similarity/cosine -similarity -calculator.php   \\n \\nGarcia, E. (2015b). A Tutorial on Distance and Similarity . Retrieved from  \\nhttp://www.minerazzi.com/tutorials/distance -similarity -tutorial.pdf  \\n \\nGarcia, E. (2015 c). The Self -Weighting Model Tutorial: Part 1 Retrieved from  \\nhttp://www.minerazzi.com/tutorials/self -weighting -model -tutorial -part-1.pdf  \\n \\nGarcia, E. (2015d ). The Self -Weighting Model Tutorial: Part 2 Retrieved from  \\nhttp://www.m inerazzi.com/tutorials/self -weighting -model -tutorial -part-2.pdf  \\n  \\nGarcia, E. (2015e). A Quantile -Quantile Plot tutorial. Retrieved from  \\nhttp://www.minerazzi.com/tutorials/quant ile-quantile -tutorial.pdf  \\n \\nGarcia, E. (2015f). The Standardizer. Retrieved from  \\nhttp://www.minerazzi.com/tools/standardizer/z -scores.php  \\n \\nRodgers, J. L., Nicewander, W. A., Toothaker, L. (1984). Linearly Independent, Orthogonal, and  \\nUncorrelated V ariables. The American Statistician , V ol. 38, No. 2. Pp 133 -134. Retrieved from  \\nhttps://pdfs.semanticscholar.org/177d/c3b719fdb7b2c01d6c341e0bd58a09a47a83.pdf   \\n \\nWikipedia (2015 a). Cosine Similarity . Retrieved from  \\nhttp://en.wikipedia.org/wiki/Cosi ne_similarity  \\n \\nWikipedia (2015b). Uncorrelated. Retrieved from  \\nhttp://en.wikipedia.org/wiki/Uncorrelated  \\nView publication stats', metadata={'source': 'cosine-similarity.pdf', 'page': 7})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52af593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))   #Checking no. of pages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1fa4d5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Cosine Similarity  T utorial  \\n \\nAbstract – This is a tutorial on the cosine similarity measure . Its meaning in the context of uncorrelated and orthogonal \\nvariables is examined.  \\n \\nKeywords: cosine similarity, tutorial, dot products, vectors , orthogonal, uncorrelated  \\nPublished : 04-10-2015 ; Update d: 08-26-2018  \\n© Edel  Garcia , PhD ; admin@minerazzi.com   \\n \\nIntroduction  \\nIn previous tutorials we discussed  the difference between distance and similarit y measures  and the \\nrisks of arbitrarily transforming or averaging these ( Garcia, 2015a;  2015b;  2015c;  2015d ).  \\nWe mentioned that  a Pearson’ s Correlation Coefficient  (r) computed from mean -centered \\nvariables , or from z-scores , is a cosine similarity . In this case , r can be computed from the \\nregression curve  slope .  \\nWe also  mentioned that l ike correlations , slopes, and cosines,  cosine similarity measures are \\nnot additive  so we cannot compute  arithmetic average s from any of these measures . The same can \\nbe said about standard deviations, rates, and dissimilar ratios. It should be noted that a mean value \\nis not an estimate of central tendency when a distribu tion is either skewed or Cauchy . Furthermore,  \\nthe Law of Large Numbers  does not apply  to a Cauchy Distribution . \\n \\nConventions used in this tutorial  \\nThis tutorial was written as  a companion for our Cosine Similarity Calculator (Garcia, 2015a).  \\nIt may serve as a basic tutorial for students and those  starting in data mining  and information \\nretrieval. F or the sake of clarity , we adopt the following conventions:  \\n \\n\\uf0b7 a and b are th e    and     vectors.  \\n\\uf0b7 dpab is the         dot product between a and b. \\n\\uf0b7 dpaa is the dot product of a with itself.  \\n\\uf0b7 dpbb is the dot product of b with itself . \\n\\uf0b7 la and lb are the      and       vector lengths .  \\n\\uf0b7    and    are unit vectors; i.e.,  la = lb = 1. ', metadata={'source': 'cosine-similarity.pdf', 'page': 1})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1]    #Checking pagewise content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf534baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/327248753\\nCosine Similarity T utorial\\nArticle  · April 2015\\nCITATIONS\\n48READS\\n2,081\\n1 author:\\nEdel Gar cia\\nMiner azzi. com\\n15 PUBLICA TIONS \\xa0\\xa0\\xa069 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Edel Gar cia on 27 A ugust 2018.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', metadata={'source': 'cosine-similarity.pdf', 'page': 0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9785262c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cosine Similarity  T utorial  \\n \\nAbstract – This is a tutorial on the cosine similarity measure . Its meaning in the context of uncorrelated and orthogonal \\nvariables is examined.  \\n \\nKeywords: cosine similarity, tutorial, dot products, vectors , orthogonal, uncorrelated  \\nPublished : 04-10-2015 ; Update d: 08-26-2018  \\n© Edel  Garcia , PhD ; admin@minerazzi.com   \\n \\nIntroduction  \\nIn previous tutorials we discussed  the difference between distance and similarit y measures  and the \\nrisks of arbitrarily transforming or averaging these ( Garcia, 2015a;  2015b;  2015c;  2015d ).  \\nWe mentioned that  a Pearson’ s Correlation Coefficient  (r) computed from mean -centered \\nvariables , or from z-s'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1].page_content[0:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01253971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'cosine-similarity.pdf', 'page': 1}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a0eca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebb3b2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/327248753\\nCosine Similarity T utorial\\nArticle  · April 2015\\nCITATIONS\\n48READS\\n2,081\\n1 author:\\nEdel Gar cia\\nMiner azzi. com\\n15 PUBLICA TIONS \\xa0\\xa0\\xa069 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Edel Gar cia on 27 A ugust 2018.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.Cosine Similarity  T utorial  \\n \\nAbstract – This is a tutorial on the cosine similarity measure . Its meaning in the context of uncorrelated and orthogonal \\nvariables is examined.  \\n \\nKeywords: cosine similarity, tutorial, dot products, vectors , orthogonal, uncorrelated  \\nPublished : 04-10-2015 ; Update d: 08-26-2018  \\n© Edel  Garcia , PhD ; admin@minerazzi.com   \\n \\nIntroduction  \\nIn previous tutorials we discussed  the difference between distance and similarit y measures  and the \\nrisks of arbitrarily transforming or averaging these ( Garcia, 2015a;  2015b;  2015c;  2015d ).  \\nWe mentioned that  a Pearson’ s Correlation Coefficient  (r) computed from mean -centered \\nvariables , or from z-scores , is a cosine similarity . In this case , r can be computed from the \\nregression curve  slope .  \\nWe also  mentioned that l ike correlations , slopes, and cosines,  cosine similarity measures are \\nnot additive  so we cannot compute  arithmetic average s from any of these measures . The same can \\nbe said about standard deviations, rates, and dissimilar ratios. It should be noted that a mean value \\nis not an estimate of central tendency when a distribu tion is either skewed or Cauchy . Furthermore,  \\nthe Law of Large Numbers  does not apply  to a Cauchy Distribution . \\n \\nConventions used in this tutorial  \\nThis tutorial was written as  a companion for our Cosine Similarity Calculator (Garcia, 2015a).  \\nIt may serve as a basic tutorial for students and those  starting in data mining  and information \\nretrieval. F or the sake of clarity , we adopt the following conventions:  \\n \\n\\uf0b7 a and b are th e    and     vectors.  \\n\\uf0b7 dpab is the         dot product between a and b. \\n\\uf0b7 dpaa is the dot product of a with itself.  \\n\\uf0b7 dpbb is the dot product of b with itself . \\n\\uf0b7 la and lb are the      and       vector lengths .  \\n\\uf0b7    and    are unit vectors; i.e.,  la = lb = 1. 2 \\n \\nInstead of just saying that the  cosine similarity between two vectors is given by the expression  \\n \\n             \\n                              (1) \\n \\nwe want to explain  what is actually scored  with (1).  \\nSimilarity is an interesting measure as there are many ways of computing it. Indeed, we built a \\ntool that computes over 70 different similarity measures  (Garcia , 2016). Since there are so many  \\nways of expressing similarity , what kind of resemblance a cosine similarity  actually scores ? This is \\nthe question that this tutorial pretends to address .  \\nGenerally speaking, similarity is a measure of resemblance; i.e., how similar or alike things \\nbeing compared are . One way  of computing similarity is through the use of vectors.   \\n \\nRepresenting data sets as vectors  \\nA matrix  is just a table filled with values . Suppose that said table consists of r number of rows and \\nc number of columns . We may refer to these as  vectors . Thus for a square matrix, one with same \\nnumber of rows and columns, row vectors  are data sets of size n = c  and column vectors  are data \\nsets of size n = r .  \\nA vector is a quantity or phenomenon  with two independent properties: direction and \\nmagnitude . For n = 2 and  n = 3, we can visualize  a vector in its n-dimensional space as a line \\nsegment ending in an arrow. The orientation of the  line segment is its direction , and the length is its \\nmagnitude.  For n >  3 we cannot visualize vectors, but we can still compute them.  \\n \\nComputing Dot Products  \\nWe may multiply a vector by itself or a nother  vector and compute a quantity called the dot p roduct  \\n(dp). This is done by multiplying vector elements  and taking summations.  To illus trate, s uppose \\nthat a and b are vectors  such that  \\n \\n\\uf0b7 a = [1, 2, 3]  \\n\\uf0b7 b = [4, -5, 6] \\n 3 \\n \\nIn this case, the dot product of  \\n \\n\\uf0b7 a with b is dpab = 1*4 + 2* -5 + 3*6 = 12  \\n\\uf0b7 a with itself is dpaa = 1*1 + 2*2 + 3*3 = 14  \\n\\uf0b7 b with itself is dpbb = 4*4 + -5*-5 + 6*6 = 77  \\n \\nWe may  now ask : What kind of information is stored in dpab, dpaa, and dpbb? \\n \\nWhat information is stored in dpab?  \\nGood question: dpab holds information about the direction  of the vectors . To be precise, if  \\n \\n\\uf0b7 dpab > 0, a and b form an angle less than 90o. \\n\\uf0b7 dpab = 0, a and b form an angle that is exactly  90o. \\n\\uf0b7 dpab < 0, a and b form an angle greater than 90o. \\n \\nTable 1 shows different types of angles. \\n \\nTable 1. Possible types of angles.  \\nType  acute  right obtuse  straight  reflex  perigon  \\nAngle , \\uf071 \\uf071\\uf020\\uf020\\uf03c\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf03d\\uf020\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf031\\uf038\\uf030\\uf020\\uf03e\\uf020\\uf071\\uf020\\uf03e\\uf020\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf020\\uf03d\\uf020\\uf031\\uf038\\uf030\\uf06f\\uf020\\uf033\\uf036\\uf030\\uf06f\\uf020\\uf03e\\uf020\\uf071\\uf020\\uf03e\\uf020\\uf020\\uf031\\uf038\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf03d\\uf020\\uf033\\uf036\\uf030\\uf06f\\uf020\\n \\nAn angle of 0o means that cos \\uf071 = 1 so the vectors point to  identical directions. A n angle of 90o \\nmeans that cos \\uf071 = 0 so t he vectors point to  perpendicular directions or are orthogonal .  \\n \\nWhat information is stored in dpaa and dpbb? \\nTaking square roots, it is clear that  dpaa and dpbb hold length informatio n. So \\n \\n\\uf0b7 la = (dpaa)½ = (14)½ = 3.74     ; i.e., the length of a. \\n\\uf0b7 lb = (dpbb)½ = (77)½ = 8.77    ; i.e., the length of  b. \\n\\uf0b7 la*lb = (dpaa)½ * (dpbb)½ = 32.83   ; i.e., the length product (lpab) of a and b. 4 \\n \\nIn other words,  (1) is a dot product/length product ratio  \\n \\n370=843212= = ..=θcosdp dpdp\\nlpdp\\nbb aaab\\nabab\\n              (2) \\n \\nTherefore, when we compute a cosine similarity  we are measuring the direction -lengt h  \\nresemblance between data sets  represented as vectors .  \\n \\nComparing V ectors of Different Lengths  \\nTo compare vectors  of different lengths , these can be recomputed  as unit vectors . A unit vector  is \\ncomputed  by dividing its elements by its  length. In other words, we are re  the previous vectors  as \\n \\n    = [1/3.74, 2/3.74, 3/3.74] \\n    = [4/8.77, -5/8.77, 6/8.77 ]  \\n \\nwhere the  hat (^) denotes a unit vector . Since the new lengths are equal to 1, the cosine similarity \\nbetween     and     is their dot product ; hence   \\n \\n  \\n370= = . θcos dpbˆaˆ                     (3) \\n \\nExpressions (1), (2), and (3) return  the same result, confirming that a cosine  is a judgment of  \\nthe orient ation of the vectors , independent of their lengths  (Wikipedia, 2015 a). \\n \\nWhat does cos \\uf071 = 0 mean ? \\nAs mentioned before, an angle of 90o means that cos \\uf071 = 0 so the vectors are perpendicular or \\northogonal. This does not necessarily mean that the variables are uncorrelated .  \\nAccording to Rodgers, Nicewander, & Toothaker (1984)  when referring to variables,  \\northogonal denotes  that raw variables are perpe ndicular while  uncorrelated  means  that centered  \\nvariables are perpendicular. Centered variables are those  with their mean removed, so  they have \\nzero mean.  5 \\n \\nIt turns out that un like vector length normalization, subtracting the mean from raw variables  \\ncan change the angle between the  vectors . The following can happen to the raw variables :  \\n \\n\\uf0b7 If they are perpendicular , can become not perpendicular so they are orthogonal, but not \\nuncorrelated.  \\n\\uf0b7 If they are not perpendicular , can become perpendicular so they are uncorrelated, but not \\northogonal.  \\n\\uf0b7 If they are perpendicular  and remain perpendicular , they are orthogonal and uncorrelated.  \\n\\uf0b7 If they are not perpendicular and remain not perpendicular , they are neither orthogonal nor \\nuncorrelated —although their angle can change . \\n \\nThe following figure , adapted from Rodgers et. al  (1984 ), illustrates these relationships .  \\nNotice that not all uncorrelated variables are orthogonal and vice versa  (Wikipedia, 2015b). The \\nfigure also shows that while all uncorrelated variables are independent, the reverse is not true.  \\nTherefore, if a textbook states that orthogo nal variables are uncorrelated, it probab ly refers to \\npaired random variables with  zero mean; i.e., to the overlapping region shown in the figure.   \\n \\n \\n \\n \\nFigure 1. Venn Diagram for  linearly  independent , uncorrelated, and orthogonal variables . \\n \\n \\n6 \\n \\nThese results are not surprising. It can be shown for centered variables that  \\n \\nr = cos \\uf071                       (4) \\n \\nBecause  r is the covariance  of paired variables n ormalize d by their standard  deviation,  \\n \\n \\nsscov\\nbaabr=                       (5) \\n \\nit must follow for centered  variables that  if cos \\uf071 = 0, then r = 0 and cov ab = 0.  \\nThe relationship r = cos \\uf071 is also true for standardized variables , also known as z-scores . These \\nare centered variables normali zed by their standard deviation. Transforming variables into z-score s \\nis, though not always, useful and recommended.   \\nOnce in a z-score format, we can run other types of tests on the variables like a quantile - \\nquantile analysis (Garcia, 2015e), or even compute r, and therefore cos \\uf071, from the slope of the \\nregression curve of  z-scores . For those inter ested , we have developed  another  \\ntool called the Standardizer (Garcia, 2015f). This tool was initially thought of as an x-to-z-score \\nstandardizer, hence its name. It is now a versatile statistical tool for univariate analysis.  \\n \\nConclusion  \\nUnlike other similarity measures , a cosine similarity is a measure of the direction -length  \\nresemblance between vectors.   \\nAn angle of 0o means that cos \\uf071 = 1 and that the vectors are oriented in identical directions; i.e.,  \\nthat the corresponding data sets  are completely similar to one another.  An angle of 90o means that \\ncos \\uf071 = 0 and that the corresponding variables are perpendicular , but not necessarily that are \\nuncorrelated .  \\nComputing cos \\uf071 from raw and center variables are two different things. T his fact  \\ncan be used to examine relationships between paired variables . Regardless of the method used to \\ncompute cosine similar ities, and for the sake of transparency , it is always a good idea to state \\nwhether raw or centered variables were used and why .  7 \\n \\nReferences  \\nGarcia, E. (2016 ). Binary Similarity Calculator. Retrieved from  \\nhttp://www.minerazzi.com/tools/s imilarity/binary -similarity -calculator.php   \\n \\nGarcia, E. (2015a) . Cosine Similarity Cal culator. Retrieved from  \\nhttp://www.minerazzi.com/tools/cosine -similarity/cosine -similarity -calculator.php   \\n \\nGarcia, E. (2015b). A Tutorial on Distance and Similarity . Retrieved from  \\nhttp://www.minerazzi.com/tutorials/distance -similarity -tutorial.pdf  \\n \\nGarcia, E. (2015 c). The Self -Weighting Model Tutorial: Part 1 Retrieved from  \\nhttp://www.minerazzi.com/tutorials/self -weighting -model -tutorial -part-1.pdf  \\n \\nGarcia, E. (2015d ). The Self -Weighting Model Tutorial: Part 2 Retrieved from  \\nhttp://www.m inerazzi.com/tutorials/self -weighting -model -tutorial -part-2.pdf  \\n  \\nGarcia, E. (2015e). A Quantile -Quantile Plot tutorial. Retrieved from  \\nhttp://www.minerazzi.com/tutorials/quant ile-quantile -tutorial.pdf  \\n \\nGarcia, E. (2015f). The Standardizer. Retrieved from  \\nhttp://www.minerazzi.com/tools/standardizer/z -scores.php  \\n \\nRodgers, J. L., Nicewander, W. A., Toothaker, L. (1984). Linearly Independent, Orthogonal, and  \\nUncorrelated V ariables. The American Statistician , V ol. 38, No. 2. Pp 133 -134. Retrieved from  \\nhttps://pdfs.semanticscholar.org/177d/c3b719fdb7b2c01d6c341e0bd58a09a47a83.pdf   \\n \\nWikipedia (2015 a). Cosine Similarity . Retrieved from  \\nhttp://en.wikipedia.org/wiki/Cosi ne_similarity  \\n \\nWikipedia (2015b). Uncorrelated. Retrieved from  \\nhttp://en.wikipedia.org/wiki/Uncorrelated  \\nView publication stats')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [Document(page_content=text)]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0907c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
    "\n",
    "\n",
    "# llm= AzureChatOpenAI(deployment_name= 'chatllm16k',  \n",
    "#                      openai_api_version=\"2023-03-15-preview\", \n",
    "#                      model_name='gpt-3.5-turbo',\n",
    "#                      temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92e073b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Write a concise and short summary of the following document.\n",
    "Doc: `{text}`\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4cb7aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "30883ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='stuff',\n",
    "    prompt=prompt,\n",
    "    verbose=False\n",
    ")\n",
    "output_summary = chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99303947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This document is a tutorial on the cosine similarity measure. It explains the meaning of cosine similarity in the context of uncorrelated and orthogonal variables. The tutorial also discusses the computation of dot products and the representation of data sets as vectors. It concludes by highlighting the difference between raw and centered variables in computing cosine similarities.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3159291",
   "metadata": {},
   "source": [
    "### Map Reduce With Custom Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c5c79e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_prompt=\"\"\"\n",
    "Please summarize the below doc:\n",
    "doc:`{text}'\n",
    "Summary:\n",
    "\"\"\"\n",
    "map_prompt_template=PromptTemplate(input_variables=['text'],\n",
    "                                    template=chunks_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "850fae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combine_prompt='''\n",
    "Provide a final summary of the entire doc with these important points.\n",
    "Add a Generic Motivational Title,\n",
    "Start the precise summary with an introduction and provide the\n",
    "summary in number points for the doc.\n",
    "doc: `{text}`\n",
    "'''\n",
    "final_combine_prompt_template=PromptTemplate(input_variables=['text'],\n",
    "                                             template=final_combine_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1fcbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65009b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splittting the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
    "chunks = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "038602ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=map_prompt_template,\n",
    "    combine_prompt=final_combine_prompt_template,\n",
    "    verbose=False\n",
    ")\n",
    "output = summary_chain.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "37cd321a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: \"Unveiling the Power of Cosine Similarity: A Comprehensive Tutorial\"\\n\\nIntroduction:\\nThis document serves as a tutorial on the concept of cosine similarity, specifically focusing on its significance in the context of uncorrelated and orthogonal variables. It provides clarity on the conventions used throughout the tutorial, offers practical examples of computing dot products, explains the information stored in dot products, and introduces the concept of unit vectors. Additionally, it explores the comparison of vectors with different lengths and delves into the relationship between orthogonal and uncorrelated variables. The document concludes by emphasizing the essence of cosine similarity as a measure of direction-length resemblance between vectors.\\n\\nSummary:\\n\\n1. Definition and Significance: The document introduces cosine similarity as a measure that evaluates the resemblance in direction and length between vectors, particularly in the context of uncorrelated and orthogonal variables.\\n\\n2. Computation and Dot Products: The tutorial provides explicit examples of calculating dot products, shedding light on the mechanics behind cosine similarity.\\n\\n3. Information in Dot Products: It explains the information stored in dot products, highlighting their relevance in determining the similarity between vectors.\\n\\n4. Unit Vectors and Comparison of Vectors: The document elucidates the concept of unit vectors and explores the comparison of vectors with varying lengths.\\n\\n5. Relationship between Orthogonal and Uncorrelated Variables: It discusses the interplay between orthogonal and uncorrelated variables, offering insights into their connections and distinctions.\\n\\n6. References to Resources: The tutorial includes a compilation of references to various resources, such as binary similarity and cosine similarity calculators, tutorials on distance and similarity, the self-weighting model, and a quantile-quantile plot. It also mentions \"The Standardizer,\" a tool for calculating z-scores.\\n\\n7. Importance of Transparency: Emphasizing the significance of transparency, the document stresses the need to specify whether raw or centered variables were employed, providing rationale behind this requirement.\\n\\n8. Additional References: The document directs readers to a research paper addressing linearly independent, orthogonal, and uncorrelated variables. It also provides links to Wikipedia articles on cosine similarity and uncorrelated variables.\\n\\nIn conclusion, this comprehensive tutorial unravels the intricacies of cosine similarity, equipping readers with a profound understanding of its applications and implications in evaluating the resemblance between vectors.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6f17d",
   "metadata": {},
   "source": [
    "### Summarizing Large Documents Using Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c90d7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splittting the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
    "chunks = text_splitter.create_documents([str(docs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "900b8293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a8d89e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=False\n",
    ")\n",
    "summary = chain.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b03fe52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This tutorial explains the concept of cosine similarity as a measure of similarity between vectors. It discusses the meaning of cosine similarity in relation to uncorrelated and orthogonal variables, and explains how to compute dot products and compare vectors of different lengths. The tutorial also explores the relationship between cosine similarity and correlation coefficients, and highlights the difference between orthogonal and uncorrelated variables. It concludes by mentioning additional tools for data analysis and provides references for further reading.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1676cd",
   "metadata": {},
   "source": [
    "### RefineChain For Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50eba078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"[Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/327248753\\nCosine Similarity T utorial\\nArticle  · April 2015\\nCITATIONS\\n48READS\\n2,081\\n1 author:\\nEdel Gar cia\\nMiner azzi. com\\n15 PUBLICA TIONS \\xa0\\xa0\\xa069 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Edel Gar cia on 27 A ugust 2018.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.Cosine Similarity  T utorial  \\n \\nAbstract – This is a tutorial on the cosine similarity measure . Its meaning in the context of uncorrelated and orthogonal \\nvariables is examined.  \\n \\nKeywords: cosine similarity, tutorial, dot products, vectors , orthogonal, uncorrelated  \\nPublished : 04-10-2015 ; Update d: 08-26-2018  \\n© Edel  Garcia , PhD ; admin@minerazzi.com   \\n \\nIntroduction  \\nIn previous tutorials we discussed  the difference between distance and similarit y measures  and the \\nrisks of arbitrarily transforming or averaging these ( Garcia, 2015a;  2015b;  2015c;  2015d ).  \\nWe mentioned that  a Pearson’ s Correlation Coefficient  (r) computed from mean -centered \\nvariables , or from z-scores , is a cosine similarity . In this case , r can be computed from the \\nregression curve  slope .  \\nWe also  mentioned that l ike correlations , slopes, and cosines,  cosine similarity measures are \\nnot additive  so we cannot compute  arithmetic average s from any of these measures . The same can \\nbe said about standard deviations, rates, and dissimilar ratios. It should be noted that a mean value \\nis not an estimate of central tendency when a distribu tion is either skewed or Cauchy . Furthermore,  \\nthe Law of Large Numbers  does not apply  to a Cauchy Distribution . \\n \\nConventions used in this tutorial  \\nThis tutorial was written as  a companion for our Cosine Similarity Calculator (Garcia, 2015a).  \\nIt may serve as a basic tutorial for students and those  starting in data mining  and information \\nretrieval. F or the sake of clarity , we adopt the following conventions:  \\n \\n\\uf0b7 a and b are th e    and     vectors.  \\n\\uf0b7 dpab is the         dot product between a and b. \\n\\uf0b7 dpaa is the dot product of a with itself.  \\n\\uf0b7 dpbb is the dot product of b with itself . \\n\\uf0b7 la and lb are the      and       vector lengths .  \\n\\uf0b7    and    are unit vectors; i.e.,  la = lb = 1. 2 \\n \\nInstead of just saying that the  cosine similarity between two vectors is given by the expression  \\n \\n             \\n                              (1) \\n \\nwe want to explain  what is actually scored  with (1).  \\nSimilarity is an interesting measure as there are many ways of computing it. Indeed, we built a \\ntool that computes over 70 different similarity measures  (Garcia , 2016). Since there are so many  \\nways of expressing similarity , what kind of resemblance a cosine similarity  actually scores ? This is \\nthe question that this tutorial pretends to address .  \\nGenerally speaking, similarity is a measure of resemblance; i.e., how similar or alike things \\nbeing compared are . One way  of computing similarity is through the use of vectors.   \\n \\nRepresenting data sets as vectors  \\nA matrix  is just a table filled with values . Suppose that said table consists of r number of rows and \\nc number of columns . We may refer to these as  vectors . Thus for a square matrix, one with same \\nnumber of rows and columns, row vectors  are data sets of size n = c  and column vectors  are data \\nsets of size n = r .  \\nA vector is a quantity or phenomenon  with two independent properties: direction and \\nmagnitude . For n = 2 and  n = 3, we can visualize  a vector in its n-dimensional space as a line \\nsegment ending in an arrow. The orientation of the  line segment is its direction , and the length is its \\nmagnitude.  For n >  3 we cannot visualize vectors, but we can still compute them.  \\n \\nComputing Dot Products  \\nWe may multiply a vector by itself or a nother  vector and compute a quantity called the dot p roduct  \\n(dp). This is done by multiplying vector elements  and taking summations.  To illus trate, s uppose \\nthat a and b are vectors  such that  \\n \\n\\uf0b7 a = [1, 2, 3]  \\n\\uf0b7 b = [4, -5, 6] \\n 3 \\n \\nIn this case, the dot product of  \\n \\n\\uf0b7 a with b is dpab = 1*4 + 2* -5 + 3*6 = 12  \\n\\uf0b7 a with itself is dpaa = 1*1 + 2*2 + 3*3 = 14  \\n\\uf0b7 b with itself is dpbb = 4*4 + -5*-5 + 6*6 = 77  \\n \\nWe may  now ask : What kind of information is stored in dpab, dpaa, and dpbb? \\n \\nWhat information is stored in dpab?  \\nGood question: dpab holds information about the direction  of the vectors . To be precise, if  \\n \\n\\uf0b7 dpab > 0, a and b form an angle less than 90o. \\n\\uf0b7 dpab = 0, a and b form an angle that is exactly  90o. \\n\\uf0b7 dpab < 0, a and b form an angle greater than 90o. \\n \\nTable 1 shows different types of angles. \\n \\nTable 1. Possible types of angles.  \\nType  acute  right obtuse  straight  reflex  perigon  \\nAngle , \\uf071 \\uf071\\uf020\\uf020\\uf03c\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf03d\\uf020\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf031\\uf038\\uf030\\uf020\\uf03e\\uf020\\uf071\\uf020\\uf03e\\uf020\\uf020\\uf039\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf020\\uf03d\\uf020\\uf031\\uf038\\uf030\\uf06f\\uf020\\uf033\\uf036\\uf030\\uf06f\\uf020\\uf03e\\uf020\\uf071\\uf020\\uf03e\\uf020\\uf020\\uf031\\uf038\\uf030\\uf06f\\uf020\\uf071\\uf020\\uf03d\\uf020\\uf033\\uf036\\uf030\\uf06f\\uf020\\n \\nAn angle of 0o means that cos \\uf071 = 1 so the vectors point to  identical directions. A n angle of 90o \\nmeans that cos \\uf071 = 0 so t he vectors point to  perpendicular directions or are orthogonal .  \\n \\nWhat information is stored in dpaa and dpbb? \\nTaking square roots, it is clear that  dpaa and dpbb hold length informatio n. So \\n \\n\\uf0b7 la = (dpaa)½ = (14)½ = 3.74     ; i.e., the length of a. \\n\\uf0b7 lb = (dpbb)½ = (77)½ = 8.77    ; i.e., the length of  b. \\n\\uf0b7 la*lb = (dpaa)½ * (dpbb)½ = 32.83   ; i.e., the length product (lpab) of a and b. 4 \\n \\nIn other words,  (1) is a dot product/length product ratio  \\n \\n370=843212= = ..=θcosdp dpdp\\nlpdp\\nbb aaab\\nabab\\n              (2) \\n \\nTherefore, when we compute a cosine similarity  we are measuring the direction -lengt h  \\nresemblance between data sets  represented as vectors .  \\n \\nComparing V ectors of Different Lengths  \\nTo compare vectors  of different lengths , these can be recomputed  as unit vectors . A unit vector  is \\ncomputed  by dividing its elements by its  length. In other words, we are re  the previous vectors  as \\n \\n    = [1/3.74, 2/3.74, 3/3.74] \\n    = [4/8.77, -5/8.77, 6/8.77 ]  \\n \\nwhere the  hat (^) denotes a unit vector . Since the new lengths are equal to 1, the cosine similarity \\nbetween     and     is their dot product ; hence   \\n \\n  \\n370= = . θcos dpbˆaˆ                     (3) \\n \\nExpressions (1), (2), and (3) return  the same result, confirming that a cosine  is a judgment of  \\nthe orient ation of the vectors , independent of their lengths  (Wikipedia, 2015 a). \\n \\nWhat does cos \\uf071 = 0 mean ? \\nAs mentioned before, an angle of 90o means that cos \\uf071 = 0 so the vectors are perpendicular or \\northogonal. This does not necessarily mean that the variables are uncorrelated .  \\nAccording to Rodgers, Nicewander, & Toothaker (1984)  when referring to variables,  \\northogonal denotes  that raw variables are perpe ndicular while  uncorrelated  means  that centered  \\nvariables are perpendicular. Centered variables are those  with their mean removed, so  they have \\nzero mean.  5 \\n \\nIt turns out that un like vector length normalization, subtracting the mean from raw variables  \\ncan change the angle between the  vectors . The following can happen to the raw variables :  \\n \\n\\uf0b7 If they are perpendicular , can become not perpendicular so they are orthogonal, but not \\nuncorrelated.  \\n\\uf0b7 If they are not perpendicular , can become perpendicular so they are uncorrelated, but not \\northogonal.  \\n\\uf0b7 If they are perpendicular  and remain perpendicular , they are orthogonal and uncorrelated.  \\n\\uf0b7 If they are not perpendicular and remain not perpendicular , they are neither orthogonal nor \\nuncorrelated —although their angle can change . \\n \\nThe following figure , adapted from Rodgers et. al  (1984 ), illustrates these relationships .  \\nNotice that not all uncorrelated variables are orthogonal and vice versa  (Wikipedia, 2015b). The \\nfigure also shows that while all uncorrelated variables are independent, the reverse is not true.  \\nTherefore, if a textbook states that orthogo nal variables are uncorrelated, it probab ly refers to \\npaired random variables with  zero mean; i.e., to the overlapping region shown in the figure.   \\n \\n \\n \\n \\nFigure 1. Venn Diagram for  linearly  independent , uncorrelated, and orthogonal variables . \\n \\n \\n6 \\n \\nThese results are not surprising. It can be shown for centered variables that  \\n \\nr = cos \\uf071                       (4) \\n \\nBecause  r is the covariance  of paired variables n ormalize d by their standard  deviation,  \\n \\n \\nsscov\\nbaabr=                       (5) \\n \\nit must follow for centered  variables that  if cos \\uf071 = 0, then r = 0 and cov ab = 0.  \\nThe relationship r = cos \\uf071 is also true for standardized variables , also known as z-scores . These \\nare centered variables normali zed by their standard deviation. Transforming variables into z-score s \\nis, though not always, useful and recommended.   \\nOnce in a z-score format, we can run other types of tests on the variables like a quantile - \\nquantile analysis (Garcia, 2015e), or even compute r, and therefore cos \\uf071, from the slope of the \\nregression curve of  z-scores . For those inter ested , we have developed  another  \\ntool called the Standardizer (Garcia, 2015f). This tool was initially thought of as an x-to-z-score \\nstandardizer, hence its name. It is now a versatile statistical tool for\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The document is a tutorial on cosine similarity, a measure of similarity between vectors. It explains the computation of dot products and how cosine similarity is measured. It also discusses the difference between orthogonal and uncorrelated variables and their relationship to cosine similarity. The tutorial provides conventions and examples to illustrate the concepts.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "tool for univariate analysis.  \\n \\nConclusion  \\nUnlike other similarity measures , a cosine similarity is a measure of the direction -length  \\nresemblance between vectors.   \\nAn angle of 0o means that cos \\uf071 = 1 and that the vectors are oriented in identical directions; i.e.,  \\nthat the corresponding data sets  are completely similar to one another.  An angle of 90o means that \\ncos \\uf071 = 0 and that the corresponding variables are perpendicular , but not necessarily that are \\nuncorrelated .  \\nComputing cos \\uf071 from raw and center variables are two different things. T his fact  \\ncan be used to examine relationships between paired variables . Regardless of the method used to \\ncompute cosine similar ities, and for the sake of transparency , it is always a good idea to state \\nwhether raw or centered variables were used and why .  7 \\n \\nReferences  \\nGarcia, E. (2016 ). Binary Similarity Calculator. Retrieved from  \\nhttp://www.minerazzi.com/tools/s imilarity/binary -similarity -calculator.php   \\n \\nGarcia, E. (2015a) . Cosine Similarity Cal culator. Retrieved from  \\nhttp://www.minerazzi.com/tools/cosine -similarity/cosine -similarity -calculator.php   \\n \\nGarcia, E. (2015b). A Tutorial on Distance and Similarity . Retrieved from  \\nhttp://www.minerazzi.com/tutorials/distance -similarity -tutorial.pdf  \\n \\nGarcia, E. (2015 c). The Self -Weighting Model Tutorial: Part 1 Retrieved from  \\nhttp://www.minerazzi.com/tutorials/self -weighting -model -tutorial -part-1.pdf  \\n \\nGarcia, E. (2015d ). The Self -Weighting Model Tutorial: Part 2 Retrieved from  \\nhttp://www.m inerazzi.com/tutorials/self -weighting -model -tutorial -part-2.pdf  \\n  \\nGarcia, E. (2015e). A Quantile -Quantile Plot tutorial. Retrieved from  \\nhttp://www.minerazzi.com/tutorials/quant ile-quantile -tutorial.pdf  \\n \\nGarcia, E. (2015f). The Standardizer. Retrieved from  \\nhttp://www.minerazzi.com/tools/standardizer/z -scores.php  \\n \\nRodgers, J. L., Nicewander, W. A., Toothaker, L. (1984). Linearly Independent, Orthogonal, and  \\nUncorrelated V ariables. The American Statistician , V ol. 38, No. 2. Pp 133 -134. Retrieved from  \\nhttps://pdfs.semanticscholar.org/177d/c3b719fdb7b2c01d6c341e0bd58a09a47a83.pdf   \\n \\nWikipedia (2015 a). Cosine Similarity . Retrieved from  \\nhttp://en.wikipedia.org/wiki/Cosi ne_similarity  \\n \\nWikipedia (2015b). Uncorrelated. Retrieved from  \\nhttp://en.wikipedia.org/wiki/Uncorrelated  \\nView publication stats')]\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    verbose=True\n",
    ")\n",
    "output_summary = chain.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "70d18237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document is a tutorial on cosine similarity, a measure of similarity between vectors. It explains the computation of dot products and how cosine similarity is measured. It also discusses the difference between orthogonal and uncorrelated variables and their relationship to cosine similarity. The tutorial provides conventions and examples to illustrate the concepts. The tutorial emphasizes the importance of stating whether raw or centered variables were used to compute cosine similarities and provides references for further reading.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2550f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
